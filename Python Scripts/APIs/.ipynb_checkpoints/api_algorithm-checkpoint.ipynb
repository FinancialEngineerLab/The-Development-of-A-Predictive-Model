{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as mt\n",
    "import copy\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn import exceptions as skle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=skle.ConvergenceWarning)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import linear_model\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn import neural_network\n",
    "from sknn import mlp\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn import pipeline\n",
    "\n",
    "seed = 15\n",
    "path_misc_files = \"../Bond Price Predictive Model/Other/\"\n",
    "\n",
    "##imports below are for use of the R library Minerva (MINE) used in calculating MIC\n",
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "import rpy2.robjects.packages as rpackages\n",
    "import rpy2.robjects.vectors as vc\n",
    "\n",
    "##below imports allow the use of R in Python for the MIC correlation package written in R\n",
    "base = rpackages.importr('base')\n",
    "utils = rpackages.importr('utils')\n",
    "stats = rpackages.importr('stats')\n",
    "#utils.install_packages(\"devtools\")\n",
    "devtools = rpackages.importr('devtools')\n",
    "#utils.install_packages(\"minerva\")\n",
    "minerva = rpackages.importr('minerva')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports data with or without MP4 features included\n",
    "#in Half data enter C or L to select only one of C or L to include\n",
    "##remove one of the C or L feature for each ASG pair of features if they have a correlation over 0.75\n",
    "##Also prints the outputs of each correlation test to the Reserach Folder\n",
    "\n",
    "def importData(country, MP4=True, c_or_l=None, max_corr=0.75,topvars=False):    \n",
    "    path = \"../Bond Price Predictive Model/\"+country+\"/Combined_hardcoded.xlsx\"\n",
    "    ##insert code to remove columns with 0 as target variable, i.e. those that went neither up or down\n",
    "    data = pd.read_excel(path,header=0)\n",
    "    data = data[(data['AA1'] != 0) ]\n",
    "    data.loc[:,'AA1'] = [0 if x==-1 else x for x in data.loc[:,'AA1']] #-1 can not be used in voting Classifier due to it being negative\n",
    "    \n",
    "    data = adeSavGolFeatureCorrelation(data,max_corr,country)\n",
    "    if topvars == True:\n",
    "        data = data.loc[:,['Date','AA1','BC3', 'MP4C','MP4L','MP1C','I2L','FF1L','I2C','MP1L','MP2L']]\n",
    "    data = MP4Status(data,MP4,country)\n",
    "    data = removeCorL(data,c_or_l)\n",
    "            \n",
    "    return data\n",
    "\n",
    "#remove, keep, or only have MP4\n",
    "def MP4Status(data,MP4,country): \n",
    "    if MP4 == False:\n",
    "        data = data[data.columns[~data.columns.str.contains(\"MP4\")]]\n",
    "        print(\"\\n MP4 removed for {} analysis\".format(country))\n",
    "    elif MP4 == \"Only\":\n",
    "        data = data.loc[:,['Date','AA1','BC3', 'MP4C','MP4L']]\n",
    "        print(\"\\n All removed except MP4 for {} analysis\".format(country))\n",
    "    elif MP4 == True:\n",
    "        print(\"\\n MP4 kept for {} analysis\".format(country))\n",
    "    return data\n",
    "\n",
    "#remove all stats of C or L\n",
    "def removeCorL(data,c_or_l):\n",
    "    if c_or_l== \"C\":\n",
    "        data = data[data.columns[~data.columns.str.endswith(\"L\")]]\n",
    "        data = data.drop('Month',1)\n",
    "    elif c_or_l == \"L\":\n",
    "        data = data[data.columns[~data.columns.str.endswith(\"C\")]]\n",
    "        data = data.drop('Month',1)\n",
    "    return data\n",
    "    \n",
    "##Used for removing  the L statistic from L,C pairs which show high levels of corellation\n",
    "def adeSavGolFeatureCorrelation(data,max_corr,country):\n",
    "    print(\"Testing Correlation between corresponding L and C AVG statistics for each underlying datatype\")\n",
    "    under_data = ['I2','I1','GM2','GM1','FF1','MP1','MP4','MP2']\n",
    "    correlation_results = pd.DataFrame(columns=under_data, index=['Pearson','MIC'])\n",
    "    cols_to_remove = []\n",
    "    for _ud in under_data:\n",
    "        change_and_level = data[[col_name for col_name in data.columns.values if col_name.startswith(_ud)]]\n",
    "        linear_corr = sp.stats.pearsonr(change_and_level.iloc[:,0],change_and_level.iloc[:,1])[0]\n",
    "        nonlinear_corr = minerva.mine(vc.FloatVector(np.asarray(change_and_level.iloc[:,0])),vc.FloatVector(np.asarray(change_and_level.iloc[:,1])))[0][0]\n",
    "        correlation_results[_ud]= pd.Series([linear_corr, float(nonlinear_corr)],index=['Pearson','MIC'])\n",
    "        ##removing columns that contain a corellation number of 0.75\n",
    "        \n",
    "        if (linear_corr>max_corr) or (nonlinear_corr>max_corr):\n",
    "            cols_to_remove = cols_to_remove + [\"{}{}\".format(_ud,\"L\")]\n",
    "    data = data.loc[:,[_col for _col in list(data.columns.values) if _col not in cols_to_remove]]\n",
    "    \n",
    "    correlation_results.to_excel(\"../Reserach/AdeSavGol Transform/L&C correlation/{}.xlsx\".format(country),engine=\"openpyxl\")\n",
    "    print(correlation_results)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##takes in a country's dataframe. USE with unedited dataframes received from Import\n",
    "def featureLag(country_data, types=['C','L']):\n",
    "    ##inital one month lag to move features to predict next month based on this month data\n",
    "    country_data[[feature for feature in country_data.columns.values if feature not in ['Date','Month']]] = country_data[[feature for feature in country_data.columns.values if feature not in ['Date','Month']]].shift()\n",
    "    \n",
    "    days_lags={ 'FF1': 0, 'GM1': 15, 'GM2':31, 'I1':31, 'I2':0, 'MP1':0, 'MP2':0, 'MP4':0 } ##days into month until US statistic is released\n",
    "    for col_name in days_lags.keys():\n",
    "        _lag = mt.ceil(days_lags[col_name]/31)\n",
    "        for _type in types:\n",
    "            _full_col_name = \"{}{}\".format(col_name,_type)\n",
    "            if _full_col_name in country_data.columns.values:\n",
    "                country_data.loc[:,_full_col_name] = country_data.loc[:,_full_col_name].shift(_lag)\n",
    "    country_data = country_data.iloc[2:,:]\n",
    "    \n",
    "    return country_data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Algos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split a country's dataset by business cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pass in dataframe for one country\n",
    "def businessCycleSplitter(list_tuples_data_country):\n",
    "    print(\"\\n Splitting Each countries data into 3 sets defined by Mentality Cycle\")\n",
    "    list_all_country_data = []\n",
    "    for data_country in list_tuples_data_country:\n",
    "        split_data_dict = {}\n",
    "        split_data_dict['country_name'] = data_country[1]\n",
    "\n",
    "        for i in np.arange(1,4):\n",
    "            bus_cycle_col_name = [col for col in data_country[0].columns.values if col[:3]==\"BC3\"][0]\n",
    "            _temp_data = data_country[0][(data_country[0][bus_cycle_col_name] == i)]\n",
    "            _temp_data.reset_index(inplace=True, drop=True)\n",
    "            split_data_dict.update({i:_temp_data})\n",
    "        list_all_country_data = list_all_country_data + [split_data_dict]\n",
    "        \n",
    "        print(\"{} split completed\".format(data_country[1]))\n",
    "        \n",
    "    return list_all_country_data\n",
    "#keys for returned dictionary are 1,2 and 3        \n",
    "#returns a dictionary of form\n",
    "#L1Key = 1,2,3,Name L1Value = X[where BC==1], X[whereBC==2], country_abbreviation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### divide each split_dataset into class and feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes in list of dictionaries from businessCycleSplitter\n",
    "def classAndFeature(list_of_dictionaries):\n",
    "    print(\"\\n Splitting Each countries data into feature and target variable\")\n",
    "    all_country_data = {}\n",
    "    for _country_dictionary in list_of_dictionaries:\n",
    "        country_data = {}\n",
    "        for _bus_cycle in list(_country_dictionary.keys() - ['country_name']): #_bus_cycle = 1, 2 or 3 - selecting different data split from above method\n",
    "            country_data_split = {} \n",
    "            _dataset = _country_dictionary[_bus_cycle]\n",
    "            Y = _dataset.loc[:,'AA1']\n",
    "            X = _dataset.loc[:,[col for col in _dataset.columns.values if col not in ['AA1','BC3','Date']]]\n",
    "            country_data_split.update({\"Y\":Y})\n",
    "            country_data_split.update({\"X\":X})\n",
    "            country_data.update({_bus_cycle:country_data_split})\n",
    "            \n",
    "        all_country_data.update({_country_dictionary.get('country_name'): country_data} )\n",
    "        print(\"{} split completed\".format(_country_dictionary['country_name'])) \n",
    "    return all_country_data\n",
    "##Returns Dictionary of the following form\n",
    "#L1Key = Country Name, L1val=Dictionary\n",
    "#L2Key = Business cycle, L2val = Dictionary\n",
    "#L3Key = Y or X, L3val = Targets, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding best algos for each split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##For each country, for each data subset split by business cycle this finds the best N algorithim types from a range of algos\n",
    "##takes in a dictionary from ClassandFeatures.\n",
    "def testingAlgoTypes(_all_country_data,MP4,verbose=0):\n",
    "    print(\"\\n \\n \\n Testing various untrained classification algorithms on each country's seperate sub datasets \")\n",
    "    all_country_data_with_algos = copy.deepcopy(_all_country_data)\n",
    "    ##parameters for NeuralNet\n",
    "    nn_layers = [mlp.Layer('Sigmoid',units=7, name=\"Layer1\"),mlp.Layer(\"Softmax\",)]\n",
    "    nn_params = {'layers':nn_layers,'learning_momentum':0.9,'n_stable':10 ,'f_stable':0.01,'learning_rate':0.001,\n",
    "                 'learning_rule':'adadelta','random_state':seed,'n_iter':8,'batch_size':100,'warning':None,\n",
    "                 'verbose':None,'debug':False}\n",
    "    \n",
    "        \n",
    "    max_iter_params = {'max_iter':1000}\n",
    "    \n",
    "    classifiers = [LinearDiscriminantAnalysis(solver='eigen',shrinkage='auto'),\n",
    "                   linear_model.RidgeClassifier(random_state=seed),\n",
    "                   linear_model.LogisticRegression(solver='saga',penalty='l2',class_weight='balanced',random_state=seed),\n",
    "                   neighbors.KNeighborsClassifier(n_neighbors=9,weights='distance',leaf_size=20),\n",
    "                   svm.LinearSVC(class_weight='balanced',random_state=seed,dual=False),\n",
    "                   ensemble.RandomForestClassifier(n_estimators=200,min_samples_split=5,min_samples_leaf=3,max_depth=3,random_state=seed), \n",
    "                   ensemble.GradientBoostingClassifier(random_state=seed, n_estimators=200, min_samples_split=5,max_features='sqrt'),\n",
    "                   mlp.Classifier(**nn_params),\n",
    "                   linear_model.PassiveAggressiveClassifier(max_iter=1000,random_state=seed,class_weight=\"balanced\"),\n",
    "                   linear_model.SGDClassifier(max_iter=1000,random_state=seed,class_weight='balanced', penalty='l2')]\n",
    "    \n",
    "    headers = ['LDA','RC','LogR','KNN','SVM','RF','GBC', 'NN','PAC','SGD']\n",
    "    \n",
    "    for country in all_country_data_with_algos.keys():\n",
    "        df_cv_results = pd.DataFrame(columns=headers)\n",
    "        for _bus_cycle in all_country_data_with_algos[country].keys() : #iterating through the different business cycles\n",
    "            means_vars_for_clf = []\n",
    "            result_all_clf = []\n",
    "            Y_target =  all_country_data_with_algos[country][_bus_cycle].get(\"Y\")\n",
    "            X_features = all_country_data_with_algos[country][_bus_cycle].get(\"X\")\n",
    "            for _clf in classifiers:\n",
    "                ##Creating Pipelines\n",
    "                #standardizer = ('standardize',preprocessing.StandardScaler())\n",
    "                algo = ('clf',_clf)\n",
    "                steps=[]\n",
    "                #steps.append(standardizer)   \n",
    "                steps.append(algo)\n",
    "                pipeline_clf = pipeline.Pipeline(steps)\n",
    "                kfold = model_selection.KFold(n_splits =2, random_state=seed,shuffle=True)\n",
    "                result_clf = model_selection.cross_val_score(pipeline_clf, np.array(X_features), Y_target.values.ravel(), cv=kfold,n_jobs=1)\n",
    "                result_all_clf = result_all_clf + [result_clf.mean()] ##used to find top 3 methods\n",
    "                \n",
    "                means_vars_for_clf = means_vars_for_clf + [\"{0:.3g}\".format(result_clf.mean())] ##used for excel sheet\n",
    "            df_cv_results.loc[\"{}-{}\".format(country, _bus_cycle),:] = means_vars_for_clf\n",
    "            \n",
    "                ##gathering names of top three algos to be inserted into all_country_data dictionary\n",
    "            top3 = sorted(result_all_clf,reverse=True)[:3]\n",
    "            indexes_of_top_3 = [result_all_clf.index(x) for x in top3]\n",
    "            top_3_algos_by_mean = [headers[x] for x in indexes_of_top_3] ##stored as 3 letter abbreviation of algo\n",
    "            all_country_data_with_algos[country][_bus_cycle].update({\"algos\":top_3_algos_by_mean})\n",
    "        \n",
    "        if MP4 == True:\n",
    "            df_cv_results.to_excel('../Reserach/Classifier Cross Validation Scores For All Countries/All/'+country+'.xlsx',index=False)\n",
    "        if MP4 == \"Only\":\n",
    "                df_cv_results.to_excel('../Reserach/Classifier Cross Validation Scores For All Countries/Only/'+country+'.xlsx',index=False)\n",
    "        if MP4 == False:\n",
    "                df_cv_results.to_excel('../Reserach/Classifier Cross Validation Scores For All Countries/Excl/'+country+'.xlsx',index=False)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(df_cv_results)\n",
    "            print(\"\\n\")\n",
    "    saveTopThreeAlgos(all_country_data_with_algos)\n",
    "    \n",
    "    return all_country_data_with_algos\n",
    "##Returns a dict of form\n",
    "#L1Keys = country names, L1Values = dictionary.\n",
    "#L2Keys = x in [1,2,3], L2Values = Dictionary\n",
    "#L3Keys = X, Y,algos L3Values = features, targets,List of algos'3 letter abbreviation\n",
    "\n",
    "def saveTopThreeAlgos(all_country_data_with_algos):\n",
    "    df = pd.DataFrame(columns=['First', 'Second', 'Third'])\n",
    "    for country in all_country_data_with_algos.keys():\n",
    "        for BC in all_country_data_with_algos[country].keys():\n",
    "            algo_list = all_country_data_with_algos[country][BC].get('algos')\n",
    "            df.loc[\"{}{}\".format(country,BC)]=[algo_list[0],algo_list[1],algo_list[2]]\n",
    "    df.to_excel(\"../Reserach/Top 3 Classifiers for Each Country MentalityCycle/Classifiers.xlsx\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tuning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Fine Tuning parameters to for a specific model\n",
    "#Takes a dictionary output in testingAlgoTypes\n",
    "\n",
    "def fineTuneModel(_all_country_data_with_algos):\n",
    "    print(\"\\n \\n Fine Tuning Parameters for the top 3 predictive algorithms for each country for each sub dataset split by Mentality/Business Cycle \")\n",
    "    all_country_data_with_algos = copy.deepcopy(_all_country_data_with_algos)\n",
    "    algos_dict = {\"LDA\":LinearDiscriminantAnalysis(), \n",
    "                  \"RC\":linear_model.RidgeClassifier(), \n",
    "                  \"LogR\":linear_model.LogisticRegression(),\n",
    "                  \"KNN\":neighbors.KNeighborsClassifier(),\n",
    "                  \"SVM\":svm.LinearSVC(),\n",
    "                  \"RF\":ensemble.RandomForestClassifier(verbose=0),\n",
    "                  \"GBC\":ensemble.GradientBoostingClassifier(verbose=0),\n",
    "                  \"NN\": mlp.Classifier(layers= [mlp.Layer('Rectifier',units=7),mlp.Layer(\"Softmax\",)]),\n",
    "                 \"PAC\": linear_model.PassiveAggressiveClassifier(),\n",
    "                 \"SGD\":linear_model.SGDClassifier()}\n",
    "   \n",
    "    cv_folds = 3\n",
    "    n_jobs_count = np.arange(1,2)\n",
    "    results = {}\n",
    "    \n",
    "    for country in all_country_data_with_algos.keys():\n",
    "        for _bus_cycle in all_country_data_with_algos[country]:\n",
    "            X = all_country_data_with_algos[country][_bus_cycle].get(\"X\")\n",
    "            Y = all_country_data_with_algos[country][_bus_cycle].get(\"Y\")\n",
    "            all_country_data_with_algos[country][_bus_cycle].update({\"trained algos\":[]})\n",
    "            \n",
    "            for _algo in all_country_data_with_algos[country][_bus_cycle].get(\"algos\"):\n",
    "            #Possible parameters for each var Parameters\n",
    "            \n",
    "                _parameters = {}\n",
    "                \n",
    "                if _algo == \"LDA\":\n",
    "                    lda_n_components = np.arange(2,8,1)\n",
    "                    shrinkage = ['auto']\n",
    "                    \n",
    "                    lda_solver = ['lsqr', 'eigen']\n",
    "                    _parameters.update({'n_components':lda_n_components,'solver':lda_solver, 'shrinkage':shrinkage}) \n",
    "                    \n",
    "                if _algo == \"RC\":\n",
    "                    rc_class_weight = ['balanced']\n",
    "                    rc_solver = ['saga','sparse_cg','svd']\n",
    "                    alpha=np.arange(0.5,4.5,0.5)\n",
    "                    _parameters.update( {'class_weight': rc_class_weight, 'solver':rc_solver,'alpha':alpha})\n",
    "                    \n",
    "                if _algo == \"LogR\":\n",
    "                    lr_penalty = ['l1','l2']\n",
    "                    lr_class_weight = ['balanced']\n",
    "                    lr_solver = ['liblinear']\n",
    "                    _parameters.update({'penalty':lr_penalty, 'class_weight':lr_class_weight, 'solver':lr_solver, 'random_state':[seed]})\n",
    "                    \n",
    "                if _algo ==\"KNN\":\n",
    "                    knn_neighbors = np.arange(2,13,1)\n",
    "                    knn_weights= ['uniform','distance']\n",
    "                    knn_leaf_size = np.arange(10,30,2)\n",
    "                    _parameters.update({'n_neighbors': knn_neighbors, 'weights': knn_weights, 'leaf_size': knn_leaf_size})\n",
    "\n",
    "                if _algo == \"SVM\":\n",
    "                    ##put change of kernel in after\n",
    "                    svm_weights = ['balanced']\n",
    "                    dual = [False]\n",
    "                    \n",
    "                    _parameters.update({'class_weight':svm_weights, 'dual':dual,'random_state':[seed] })\n",
    "                    \n",
    "                if _algo == \"RF\":\n",
    "                    rf_max_depth = np.arange(1,5,1)\n",
    "                    n_estimators = np.asarray([200])\n",
    "                    min_samples_leaf = np.arange(3,6,1)\n",
    "                    min_samples_split= np.arange(3,5,1)\n",
    "                    max_features = [\"sqrt\"]\n",
    "                    _parameters.update({'max_depth':rf_max_depth, 'n_estimators':n_estimators,'min_samples_leaf':min_samples_leaf,\n",
    "                                       'min_samples_split':min_samples_split,'max_features':max_features, 'random_state':[seed]})\n",
    "                    \n",
    "                if _algo == \"GBC\":\n",
    "                    gb_loss =['deviance']\n",
    "                    gb_max_depth = np.arange(1,5,1)\n",
    "                    n_estimators = np.asarray([200])\n",
    "                    min_samples_leaf = np.arange(3,6,1)\n",
    "                    min_samples_leaf = np.arange(3,6,1)\n",
    "                    min_samples_split= np.arange(3,6,1)\n",
    "                    max_features = [\"sqrt\"]\n",
    "                    _parameters.update({'loss':gb_loss, 'max_depth':gb_max_depth, 'min_samples_leaf':min_samples_leaf,\n",
    "                                       'n_estimators':n_estimators, 'min_samples_leaf':min_samples_leaf,\n",
    "                                        'max_features':max_features, 'random_state':[seed] })\n",
    "                    \n",
    "                if _algo == \"NN\":\n",
    "                    layer_1 = [mlp.Layer(type=\"Sigmoid\",units=7,name=\"layer1\"),mlp.Layer(type=\"Softmax\",name=\"layer2\")]\n",
    "                    #mlp.Layer('Rectifier',units=5)\n",
    "                    nn_layers = [layer_1]\n",
    "                    nn_regularize = ['L1']\n",
    "                    learning_rate= [0.01]\n",
    "                    n_iter = [1000]\n",
    "                    weight_decay = [0.01]\n",
    "                    learning_rule=['adadelta']\n",
    "                    momentum=[0.90]\n",
    "                    n_stable=np.arange(150,151,2)\n",
    "                    f_stable=[0.001]\n",
    "                    dropout_rate=np.asarray([0,0.25,0.5])\n",
    "                    random_state=[seed]\n",
    "                    nn_params = {'layers':nn_layers, 'regularize':nn_regularize,'learning_rate':learning_rate,\n",
    "                                 'n_iter':n_iter,'learning_rule':learning_rule,'n_iter':n_iter,'weight_decay':weight_decay, \n",
    "                                'learning_momentum':momentum,'n_stable':n_stable, 'random_state':random_state} #hidden layer size should be average of input layer and output layer\n",
    "                    _parameters.update(nn_params)             \n",
    "                \n",
    "                if _algo== \"PAC\":\n",
    "                    class_weight=['balanced']\n",
    "                    max_iter = np.arange(1000,10001,1)\n",
    "                    _parameters.update({'class_weight':class_weight,'max_iter':max_iter,'random_state':[seed]})\n",
    "\n",
    "                if _algo== \"SGD\":\n",
    "                    loss = ['squared_hinge','hinge']\n",
    "                    class_weight=['balanced']\n",
    "                    penalty = ['l2','l1','elasticnet']\n",
    "                    _parameters.update({'loss':loss, 'class_weight':class_weight, 'max_iter':[1000],'penalty':penalty,\n",
    "                                        'random_state':[seed] })\n",
    "                    \n",
    "                _grid = model_selection.GridSearchCV(algos_dict.get(_algo), param_grid=_parameters, cv=cv_folds,n_jobs=1)\n",
    "               \n",
    "   \n",
    "                _grid.fit(np.array(X), Y.as_matrix().flatten())\n",
    "                \n",
    "                trained_algo = _grid.best_estimator_\n",
    "                all_country_data_with_algos[country][_bus_cycle][\"trained algos\"].append(trained_algo)\n",
    "\n",
    "    return all_country_data_with_algos\n",
    "##Returns a dict of form\n",
    "#L1Keys = country names, L1Values = dictionary.\n",
    "#L2Keys = x in [1,2,3], L2Values = Dictionary\n",
    "#L3Keys = \"X\", \"Y\",\"algos\",\"trained algos\" L3Values = features, targets,List of algos' abbreviated,trained instance of algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing trained Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Ensemble Test: \n",
    "    ####1) Testing by division on country and BC split\n",
    "    ####2) Accumalating results of BC splits per country to get per country accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##takes output from above method. 2nd argument is the data for test country (US)\n",
    "def votingEnsembleTest(all_country_data_with_algos, test_country_data_US):\n",
    "    print(\" \\n For each training set country for each sub dataset (split by Mentality Cycle): the top n trained algorithms form a Voting Classifiers. This Voting Classifiers is then tested on its corresponding US sub data set. An aggregate scocre for each trainging set country is calculated through an Aggregation of its 3 Voting Classifiers' performances\")\n",
    "    _all_country_data_with_trained_algos = copy.deepcopy(all_country_data_with_algos)\n",
    "    \n",
    "    for country in _all_country_data_with_trained_algos.keys():\n",
    "        country_level_total_hits = 0\n",
    "        for BC in _all_country_data_with_trained_algos[country].keys():\n",
    "            classifiers = copy.deepcopy(_all_country_data_with_trained_algos[country][BC].get('trained algos'))\n",
    "            \n",
    "            clf_weights = np.asarray([1,1,1],dtype=int)\n",
    "            \n",
    "            Y = test_country_data_US[BC].get(\"Y\")\n",
    "            X = test_country_data_US[BC].get(\"X\")\n",
    "            \n",
    "            vclf = EnsembleVoteClassifier(clfs=classifiers ,weights=clf_weights ,refit=False, voting='hard') # voting='soft'            \n",
    "            \n",
    "            vclf.fit(X,Y)\n",
    "            y_estimate = vclf.predict(np.array(X))\n",
    "            print(\"Voting Classifier trained on {} Mentality Cycle {} has accuracy: {}\".format(country,BC ,np.mean(Y==pd.Series(y_estimate))))\n",
    "            \n",
    "            ##saving Country-BC split accuracy and instance of Voting Classifier score to all_country... dictionary\n",
    "            _all_country_data_with_trained_algos[country][BC]['accuracy'] = np.mean(Y==y_estimate)\n",
    "            _all_country_data_with_trained_algos[country][BC]['votingclassifier'] = vclf           \n",
    "            country_level_total_hits = country_level_total_hits + np.sum(Y==y_estimate)\n",
    "        \n",
    "        record_count = test_country_data_US[1][\"Y\"].shape[0] + test_country_data_US[2][\"Y\"].shape[0] + test_country_data_US[3][\"Y\"].shape[0]\n",
    "        _all_country_data_with_trained_algos[country]['accuracy'] = (country_level_total_hits / record_count)\n",
    "        print(\"Aggregated Classifier trained on {} has accuracy: {} \\n\".format(country,_all_country_data_with_trained_algos[country]['accuracy']))\n",
    "    \n",
    "    return _all_country_data_with_trained_algos\n",
    "##Returns a dict of form\n",
    "#L1Keys = country names, L1Values = dictionary.\n",
    "#L2Keys = x in [1,2,3], \"accuracy\" L2Values = Dictionary,Combined Accuracy across BCs per country\n",
    "#L3Keys = \"X\", \"Y\",\"algos\",\"trained algos\",\"accuracy\",\"votingclassifier\" L3Values = features, targets,List of algos' abbreviated,trained instance of algorithm,accuracy on US Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Layer Ensemble Test Combining: Ensemble of BC Ensembles by top country_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Using the accuracy scores for individual Country-BC level ensembles, I will select the following ensemble\n",
    "##Voting Classifier using top 2(maybe 3) ensembles\n",
    "def votingEnsembleTest2ndLayer(a_c_d_w_t_a_and_acc_scores,test_country_data,ensemble_size=3):\n",
    "    print(\"For each mentality cycle, the top 3 or 2 Voting Classifiers across countries are combined to form a 2nd Level Voting Classifier\")\n",
    "    a_c_d_w_t_a_and_acc_scores2 = copy.deepcopy(a_c_d_w_t_a_and_acc_scores)\n",
    "    top_ensembles_dict = votingEnsembleTest2ndLayer_baseVClassifierSelection(a_c_d_w_t_a_and_acc_scores2,ensemble_size)\n",
    "    ##null before this point\n",
    "    votingEnsembleTest2ndLayer_Test(top_ensembles_dict,test_country_data)\n",
    "    \n",
    "    \n",
    "\n",
    "def votingEnsembleTest2ndLayer_baseVClassifierSelection(a_c_d_w_t_a_and_acc_scores2,ensemble_size):\n",
    "    top_ensembles_dict = {}\n",
    "    for BC in list(a_c_d_w_t_a_and_acc_scores2['UK'].keys() - ['accuracy']):\n",
    "        top_accuracies = []\n",
    "        for country in a_c_d_w_t_a_and_acc_scores2.keys():\n",
    "            accuracy = a_c_d_w_t_a_and_acc_scores2[country][BC].get('accuracy')\n",
    "            vclf = a_c_d_w_t_a_and_acc_scores2[country][BC].get('votingclassifier')\n",
    "            \n",
    "            if len(top_accuracies)<ensemble_size :  ##top2accuracies is a list of lists of form [[country,vclf,accuracy]]\n",
    "                top_accuracies = top_accuracies + [[country,vclf,accuracy]]\n",
    "            \n",
    "            else:\n",
    "                acc_list = [acc for acc in [sub_list[2] for sub_list in top_accuracies]]\n",
    "                if accuracy > min(acc_list):\n",
    "                    index_of_min = acc_list.index(min(acc_list)) # find index of lowest accuracy\n",
    "                    top_accuracies[index_of_min] = [country,vclf,accuracy]\n",
    "        \n",
    "        top_ensembles_dict[BC] = top_accuracies\n",
    "    return top_ensembles_dict\n",
    "        ##top ensembles is a dictionary of the form:\n",
    "        #L1 keys: 1,2,3 L1Values: array of arrays of form [[country,vclf,accuracy],]\n",
    "\n",
    "def votingEnsembleTest2ndLayer_Test(top_ensembles_dict,test_country_data):\n",
    "    hit_count=0\n",
    "    for BC in top_ensembles_dict.keys():\n",
    "        classifiers= [_vclf for _vclf in [sub_list[1] for sub_list in top_ensembles_dict[BC]]]\n",
    "        _weights = np.asarray([1]*len(classifiers))\n",
    "        vclf_layer2 = EnsembleVoteClassifier(clfs=classifiers, weights=_weights,refit=False)\n",
    "        Y = test_country_data[BC][\"Y\"]\n",
    "        X = test_country_data[BC][\"X\"]\n",
    "        vclf_layer2.fit(X,Y)\n",
    "        y_estimate = vclf_layer2.predict(X)\n",
    "        print(\"Mentality Cycle {} 2nd Layer Voting Classifier Ensemble has accuracy: {}\".format(BC ,np.mean(Y==y_estimate)))\n",
    "        hit_count = hit_count + np.sum(Y==y_estimate) ##calc overall performance of top 3 classifiers for each region\n",
    "    \n",
    "    total_obvs = test_country_data[1][\"Y\"].shape[0]+test_country_data[2][\"Y\"].shape[0]+test_country_data[3][\"Y\"].shape[0]\n",
    "    overall_hit_rate = hit_count/total_obvs\n",
    "    print(\"Aggregated accuracy of 2nd Layer Voting Classifiers is: {}\".format(overall_hit_rate))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
