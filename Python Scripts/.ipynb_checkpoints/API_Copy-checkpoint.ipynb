{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import plotly as py\n",
    "from plotly.offline import download_plotlyjs\n",
    "py.offline.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import nbconvert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook API.ipynb to script\n",
      "[NbConvertApp] ERROR | Notebook JSON is invalid: 'execution_count' is a required property\n",
      "\n",
      "Failed validating 'required' in execute_result:\n",
      "\n",
      "On instance['cells'][0]['outputs'][0]:\n",
      "{'data': {'text/html': \"<script>requirejs.config({paths: { 'plotly': \"\n",
      "                       \"['https://cdn.plot....\"},\n",
      " 'metadata': {},\n",
      " 'output_type': 'execute_result'}\n",
      "[NbConvertApp] Writing 6870 bytes to API.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script API.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Date Filter\n",
    "def date_filter(data, start_date, end_date, replace=False):\n",
    "    df_filtered = data[(data['Date'] >= start_date) & (data['Date'] <= end_date)]\n",
    "    if replace is True:\n",
    "        data = df_filtered\n",
    "        data.reset_index(inplace=True,drop=True)\n",
    "        return data\n",
    "    return (data ,df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#xy_xx columns\n",
    "def xy_xx_coloumns(data,cols,replace=False):\n",
    "    i = 0\n",
    "    temp_xy_cols= pd.DataFrame() \n",
    "    cols_to_mult = data.loc[:,cols]\n",
    "    for n in cols:\n",
    "        m = list(cols).index(n)\n",
    "        while m < len(cols):\n",
    "            temp_xy_cols=pd.concat([temp_xy_cols,cols_to_mult.loc[:,n].mul(cols_to_mult.iloc[:,m],axis='index')],axis='columns', join='outer')\n",
    "            m = m+1\n",
    "    temp_xy_cols.columns = ['{0}-{1}'.format(cols[y],cols[z]) for y in list(range(0,len(cols))) for z in list(range(0,len(cols))) if z>=y ]\n",
    "    \n",
    "    if replace is True:\n",
    "        data = pd.concat([data,temp_xy_cols],axis='columns',join='inner')\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "        return data\n",
    "    return(data, temp_xy_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Lags\n",
    "def lags(data, cols, shifts, replace=False):\n",
    "    i=0\n",
    "    df_lags= pd.DataFrame()\n",
    "    \n",
    "    while i < shifts:\n",
    "        t = (data.loc[:,cols]).shift(i+1)\n",
    "        t_cols=list(t.columns.values)\n",
    "        t.columns = ['{0}-lags-{1}'.format(y,z+1) for z in list(range(0,3)) for y in t_cols if ((t_cols.index(y)>=20*z) & (t_cols.index(y)<20*(z+1)) ) ]\n",
    "        df_lags= pd.concat([df_lags, t],axis='columns', join='outer')\n",
    "        i = i+1\n",
    "    \n",
    "    if replace is True:\n",
    "        data = pd.concat([data,df_lags] ,axis=1,join='inner')\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "        return data\n",
    "    return (data,df_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##copys coloumn of data from excel sheet to excel sheet containing all data for that country\n",
    "##inner joins by \"YYYY-MM\"\n",
    "\n",
    "\n",
    "def innerjoin_date(dir_source_excel,dir_model_country, features, re_turn = False):\n",
    "    \n",
    "    dir_bus_US = 'C:/Users/Rilwa/Desktop/Algorthmic Trading Platform/Business Cycles/US/Data/'\n",
    "    dir_pred_UK = \"\"\n",
    "    dir_pred_JPN = \"\"\n",
    "    dirs_excel = {  'dir_bus_US':dir_bus_US,\n",
    "                    'dir_pred_UK':dir_pred_UK,\n",
    "                    'dir_pred_JPN': dir_pred_JPN\n",
    "                 }\n",
    "       \n",
    "    df_combined = pd.read_excel(dirs_excel[dir_model_country]+'Combined.xlsx',sheetname='Data',header=0,datr_parser=True)\n",
    "    \n",
    "    for feature in features:\n",
    "        df_data = pd.read_excel(dirs_excel[dir_model_country]+feature+'_hardcoded.xlsx',sheetname='Data',header=0,datr_parser=True)\n",
    "        df_date_var = df_data.loc[:,['Month_Year',feature]]  \n",
    "        df_combined = pd.merge(df_combined,df_date_var,on='Month-Year',how='left',sort=False)\n",
    "    \n",
    "    writer = pd.ExcelWriter(dirs_excel[dir_model_country]+'Combined_hardcoded.xlsx')\n",
    "    df_combined.to_excel(writer,'Data',index=False)\n",
    "    \n",
    "    if re_turn == True : \n",
    "        return df_combined.loc[:,features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Interpolation on coloumn of data\n",
    "#Use for interpolation on levels\n",
    "def Interpolation_Levels(cols,combined_excel_file_path, re_turn = False):\n",
    "    df_uninterpolated = pd.read_excel(combined_excel_file_path,sheetname='Data',header=0,datr_parser=True)\n",
    "    df_interpolated = df_uninterpolated\n",
    "    temp = pd.DataFrame.empty\n",
    "    for col in cols:\n",
    "         temp = df_uninterpolated.loc[:col].interpolate(method='linear')\n",
    "         df_interpolated.loc[:,col] = temp\n",
    "    \n",
    "    writer = pd.ExcelWriter(combined_excel_file_path+'_hardocded.xlsx')\n",
    "    df_interpolated.to_excel(writer,'Data',index=False)\n",
    "    \n",
    "    if re_turn == True:\n",
    "        return df_interpolated.loc[:,[cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Interpolation on coloumn of data\n",
    "##Interpolation on Changes: divides data points by three and spreads across the preceeding two months\n",
    "def Interpolation_Level_Changes(cols,combined_excel_file_path, re_turn = False):\n",
    "    df_uninterpolated = pd.read_excel(combined_excel_file_path+'.xlsx',sheetname='Data',header=0,datr_parser=True)\n",
    "    df_interpolated = df_uninterpolated\n",
    "    temp = pd.DataFrame.empty\n",
    "    \n",
    "    for col in cols:\n",
    "        temp = df_uninterpolated.loc[:,col]\n",
    "        temp = temp[::,-1].interpolate(method='zero', limit_direction='forward')[::-1].div(3)\n",
    "        df_interpolated.loc[:,col]= temp\n",
    "    \n",
    "    writer = pd.ExcelWriter(combined_excel_file_path+'_hardcoded.xlsx')\n",
    "    df_interpolated.to_excel(writer,'Data',index=False)\n",
    "    \n",
    "    if re_turn == True:\n",
    "        return df_interpolated.loc[:,[cols]]\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Savitzky-Golay Filters Smoothing\n",
    "##sends it from excel file to excel file\n",
    "##intended to work on raw files, to be used before sending to combined\n",
    "\n",
    "\n",
    "#Method\n",
    "##UnSavgolfiltered values are transformed by subtracting the min value from all values\n",
    "##train savgol filter on data : window size chosen to be a factor 2 bigger than polyorder\n",
    "## Finds unit value of Positivity/negativity in SAVGOl filter betwen t1 and t0.\n",
    "##This value (+1/-1) is then multiplied by the corresponding unSavgolfiltered values\n",
    "##Allows one to distinguish between high and rising and high and dipping.\n",
    "\n",
    "def Savgol_filter(out_col,dir_model_country,windowlength=9, polyorder=3,save_xlsx= False, re_turn_graph = False):\n",
    "    \n",
    "    dir_bus_US = 'C:/Users/Rilwa/Desktop/Algorthmic Trading Platform/Business Cycles/US/Data/'\n",
    "    dir_pred_UK = \"\"\n",
    "    dir_pred_JPN = \"\"\n",
    "    \n",
    "    dirs_excel = {'dir_bus_US':dir_bus_US,\n",
    "                        'dir_pred_UK':dir_pred_UK,\n",
    "                        'dir_pred_JPN': dir_pred_JPN}\n",
    "    \n",
    "    path = dirs_excel[dir_model_country]+out_col\n",
    "    \n",
    "    df = pd.read_excel(path+'.xlsx', sheetname= 'Data', parse_dates = False)\n",
    "    \n",
    "    savgol_signal = pd.DataFrame(signal.savgol_filter(df.loc[:,'norm'], window_length=windowlength, polyorder=polyorder))\n",
    "    df['norm'+\" Savgol_filtered\"] = savgol_signal\n",
    "    signal_change_per_period = savgol_signal.diff()\n",
    "    signal_change_per_period = signal_change_per_period.applymap(lambda x: 1-(2*(x<=0)))\n",
    "    df[out_col] = df['norm'] - (df['norm'].min())   \n",
    "    df[out_col] = df[out_col] * signal_change_per_period.iloc[:,0]\n",
    "    \n",
    "    if save_xlsx == True:\n",
    "        writer = pd.ExcelWriter(path+'_hardcoded.xlsx')\n",
    "        df.to_excel(writer,'Data',index=False)\n",
    "\n",
    "    if re_turn_graph == True:\n",
    "        trace0 = go.Scatter(x= np.arange(len(savgol_signal)-1),y=savgol_signal.as_matrix().flatten())\n",
    "        trace1= go.Scatter(x= np.arange(len(savgol_signal)-1), y=df['norm'].as_matrix().flatten())\n",
    "        trace2 = go.Scatter(x= np.arange(len(savgol_signal)-1), y=df[out_col].as_matrix().flatten())\n",
    "        data = [trace0,trace1,trace2]\n",
    "        layout = dict(title = out_col)\n",
    "        py.offline.iplot({\"data\":data, \"layout\": layout})\n",
    "        #py.offline.iplot({\"data\": data,\"layout\": layout})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rilwa\\Anaconda3\\python.exe\n",
      "\n",
      "['', 'C:\\\\Users\\\\Rilwa\\\\Anaconda3\\\\python35.zip', 'C:\\\\Users\\\\Rilwa\\\\Anaconda3\\\\DLLs', 'C:\\\\Users\\\\Rilwa\\\\Anaconda3\\\\lib', 'C:\\\\Users\\\\Rilwa\\\\Anaconda3', 'C:\\\\Users\\\\Rilwa\\\\Anaconda3\\\\lib\\\\site-packages', 'C:\\\\Users\\\\Rilwa\\\\Anaconda3\\\\lib\\\\site-packages\\\\Sphinx-1.5.1-py3.5.egg', 'C:\\\\Users\\\\Rilwa\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Rilwa\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Rilwa\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\Rilwa\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Rilwa\\\\.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable +'\\n')\n",
    "print(sys.path)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
