{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/plain": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating to C:\\Users\\Rilwa\\AppData\\Local\\Temp\\gen_py\\3.5\\00020813-0000-0000-C000-000000000046x0x1x9.py\n",
      "Building definitions from type library...\n",
      "Generating...\n",
      "Importing module\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "\n",
    "import plotly as py\n",
    "from plotly.offline import download_plotlyjs\n",
    "py.offline.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import quandl as qd\n",
    "qd.ApiConfig.api_key = \"o5sCYdh6rHfBtU_RVto\"\n",
    "from fredapi import Fred\n",
    "#import tradingeconomics as te\n",
    "#te.login(\"\")\n",
    "\n",
    "import win32com.client as cl\n",
    "from win32com.client import makepy\n",
    "import sys\n",
    "\n",
    "sys.argv = [\"makepy\", r\"Excel.Application.16\"]\n",
    "makepy.main()\n",
    "\n",
    "\n",
    "import datetime\n",
    "import pytz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This application is used to convert notebook files (*.ipynb) to various other\n",
      "formats.\n",
      "\n",
      "WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
      "\n",
      "Options\n",
      "\n",
      "-------\n",
      "\n",
      "\n",
      "\n",
      "Arguments that take values are actually convenience aliases to full\n",
      "Configurables, whose aliases are listed on the help line. For more information\n",
      "on full configurables, see '--help-all'.\n",
      "\n",
      "\n",
      "--stdin\n",
      "\n",
      "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
      "\n",
      "--debug\n",
      "\n",
      "    set log level to logging.DEBUG (maximize logging output)\n",
      "\n",
      "--stdout\n",
      "\n",
      "    Write notebook output to stdout instead of files.\n",
      "\n",
      "--inplace\n",
      "\n",
      "    Run nbconvert in place, overwriting the existing notebook (only \n",
      "    relevant when converting to notebook format)\n",
      "\n",
      "--generate-config\n",
      "\n",
      "    generate default config file\n",
      "\n",
      "--execute\n",
      "\n",
      "    Execute the notebook prior to export.\n",
      "\n",
      "-y\n",
      "\n",
      "    Answer yes to any questions instead of prompting.\n",
      "\n",
      "--allow-errors\n",
      "\n",
      "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
      "--post=<DottedOrNone> (NbConvertApp.postprocessor_class)\n",
      "\n",
      "    Default: ''\n",
      "\n",
      "    PostProcessor class used to write the results of the conversion\n",
      "\n",
      "--config=<Unicode> (JupyterApp.config_file)\n",
      "\n",
      "    Default: ''\n",
      "\n",
      "    Full path of a config file.\n",
      "\n",
      "--reveal-prefix=<Unicode> (SlidesExporter.reveal_url_prefix)\n",
      "\n",
      "    Default: ''\n",
      "\n",
      "    The URL prefix for reveal.js. This can be a a relative URL for a local copy\n",
      "\n",
      "    of reveal.js, or point to a CDN.\n",
      "\n",
      "    For speaker notes to work, a local reveal.js prefix must be used.\n",
      "\n",
      "--template=<Unicode> (TemplateExporter.template_file)\n",
      "\n",
      "    Default: ''\n",
      "\n",
      "    Name of the template file to use\n",
      "\n",
      "--writer=<DottedObjectName> (NbConvertApp.writer_class)\n",
      "\n",
      "    Default: 'FilesWriter'\n",
      "\n",
      "    Writer class used to write the  results of the conversion\n",
      "\n",
      "--nbformat=<Enum> (NotebookExporter.nbformat_version)\n",
      "\n",
      "    Default: 4\n",
      "\n",
      "    Choices: [1, 2, 3, 4]\n",
      "\n",
      "    The nbformat version to write. Use this to downgrade notebooks.\n",
      "\n",
      "--to=<Unicode> (NbConvertApp.export_format)\n",
      "\n",
      "    Default: 'html'\n",
      "\n",
      "    The export format to be used, either one of the built-in formats, or a\n",
      "\n",
      "    dotted object name that represents the import path for an `Exporter` class\n",
      "\n",
      "--output-dir=<Unicode> (FilesWriter.build_directory)\n",
      "\n",
      "    Default: ''\n",
      "\n",
      "    Directory to write output to.  Leave blank to output to the current\n",
      "\n",
      "    directory\n",
      "\n",
      "--log-level=<Enum> (Application.log_level)\n",
      "\n",
      "    Default: 30\n",
      "\n",
      "    Choices: (0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL')\n",
      "\n",
      "    Set the log level by value or name.\n",
      "\n",
      "--output=<Unicode> (NbConvertApp.output_base)\n",
      "\n",
      "    Default: ''\n",
      "\n",
      "    overwrite base name use for output files. can only be used when converting\n",
      "\n",
      "    one notebook at a time.\n",
      "\n",
      "To see all available configurables, use `--help-all`\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "    The simplest way to use nbconvert is\n",
      "    \n",
      "    > jupyter nbconvert mynotebook.ipynb\n",
      "    \n",
      "    which will convert mynotebook.ipynb to the default format (probably HTML).\n",
      "    \n",
      "    You can specify the export format with `--to`.\n",
      "    Options include ['custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides']\n",
      "    \n",
      "    > jupyter nbconvert --to latex mynotebook.ipynb\n",
      "    \n",
      "    Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
      "    'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You\n",
      "    can specify the flavor of the format used.\n",
      "    \n",
      "    > jupyter nbconvert --to html --template basic mynotebook.ipynb\n",
      "    \n",
      "    You can also pipe the output to stdout, rather than a file\n",
      "    \n",
      "    > jupyter nbconvert mynotebook.ipynb --stdout\n",
      "    \n",
      "    PDF is generated via latex\n",
      "    \n",
      "    > jupyter nbconvert mynotebook.ipynb --to pdf\n",
      "    \n",
      "    You can get (and serve) a Reveal.js-powered slideshow\n",
      "    \n",
      "    > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
      "    \n",
      "    Multiple notebooks can be given at the command line in a couple of \n",
      "    different ways:\n",
      "    \n",
      "    > jupyter nbconvert notebook*.ipynb\n",
      "    > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
      "    \n",
      "    or you can specify the notebooks list in a config file, containing::\n",
      "    \n",
      "        c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
      "    \n",
      "    > jupyter nbconvert --config mycfg.py\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | pattern 'API.ipynb' matched no files\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script API.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Date Filter\n",
    "def date_filter(data, start_date, end_date, replace=False):\n",
    "\n",
    "    df_filtered = data[(data['Date'] >= start_date) & (data['Date'] <= end_date)]\n",
    "    if replace is True:\n",
    "        data = df_filtered\n",
    "        data.reset_index(inplace=True,drop=True)\n",
    "        return data\n",
    "    return (data ,df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#xy_xx columns\n",
    "def xy_xx_coloumns(data,cols,replace=False):\n",
    "    i = 0\n",
    "    temp_xy_cols= pd.DataFrame() \n",
    "    cols_to_mult = data.loc[:,cols]\n",
    "    for n in cols:\n",
    "        m = list(cols).index(n)\n",
    "        while m < len(cols):\n",
    "            temp_xy_cols=pd.concat([temp_xy_cols,cols_to_mult.loc[:,n].mul(cols_to_mult.iloc[:,m],axis='index')],axis='columns', join='outer')\n",
    "            m = m+1\n",
    "    temp_xy_cols.columns = ['{0}-{1}'.format(cols[y],cols[z]) for y in list(range(0,len(cols))) for z in list(range(0,len(cols))) if z>=y ]\n",
    "    \n",
    "    if replace is True:\n",
    "        data = pd.concat([data,temp_xy_cols],axis='columns',join='inner')\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "        return data\n",
    "    return(data, temp_xy_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Lags\n",
    "def lags(data, cols, shifts, replace=False):\n",
    "    i=0\n",
    "    df_lags= pd.DataFrame()\n",
    "    \n",
    "    while i < shifts:\n",
    "        t = (data.loc[:,cols]).shift(i+1)\n",
    "        t_cols=list(t.columns.values)\n",
    "        t.columns = ['{0}-lags-{1}'.format(y,z+1) for z in list(range(0,3)) for y in t_cols if ((t_cols.index(y)>=20*z) & (t_cols.index(y)<20*(z+1)) ) ]\n",
    "        df_lags= pd.concat([df_lags, t],axis='columns', join='outer')\n",
    "        i = i+1\n",
    "    \n",
    "\n",
    "    data = pd.concat([data,df_lags] ,axis=1,join='inner')\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return data.iloc[1:,:]\n",
    "\n",
    "##copys coloumn of data from excel sheet to excel she,reet containing all data for that country\n",
    "##inner joins by \"YYYY-MM\"\n",
    "def innerjoin_date(dir_model_country, features, re_turn = False):\n",
    "\n",
    "    dir_bus_US = 'C:/Users/Rilwa/Desktop/Algorthmic Trading Platform/Business Cycles/US/Data/'\n",
    "    dir_pred_UK = \"\"\n",
    "    dir_pred_JPN = \"\"\n",
    "    dirs_excel = {  'dir_bus_US':dir_bus_US,\n",
    "                    'dir_pred_UK':dir_pred_UK,\n",
    "                    'dir_pred_JPN': dir_pred_JPN\n",
    "                 }\n",
    "       \n",
    "    df_combined = pd.read_excel(dirs_excel[dir_model_country]+'Combined.xlsx',sheetname='Data',header=0,datr_parser=True)\n",
    "    \n",
    "    for feature in features:\n",
    "        df_data = pd.read_excel(dirs_excel[dir_model_country]+feature+'_hardcoded.xlsx',sheetname='Data',header=0,datr_parser=True)\n",
    "        df_date_var = df_data.loc[:,['Month-Year',feature]]\n",
    "        df_combined = pd.merge(df_combined,df_date_var,on='Month-Year',how='left',sort=False)\n",
    "    \n",
    "    writer = pd.ExcelWriter(dirs_excel[dir_model_country]+'Combined_hardcoded.xlsx')\n",
    "    df_combined.to_excel(writer,'Data',index=False)\n",
    "    \n",
    "    if re_turn == True : \n",
    "        return df_combined.loc[:,features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Interpolation on coloumn of data\n",
    "#Use for interpolation on levels\n",
    "def Interpolation_Levels(cols,dir_model_country, re_turn = False):\n",
    "\n",
    "    dir_bus_US = 'C:/Users/Rilwa/Desktop/Algorthmic Trading Platform/Business Cycles/US/Data/'\n",
    "    dir_pred_UK = \"\"\n",
    "    dir_pred_JPN = \"\"\n",
    "    dirs_excel = {'dir_bus_US': dir_bus_US,\n",
    "                  'dir_pred_UK': dir_pred_UK,\n",
    "                  'dir_pred_JPN': dir_pred_JPN\n",
    "                  }\n",
    "\n",
    "    df_interpolated = pd.read_excel(dirs_excel[dir_model_country]+'Combined_hardcoded.xlsx',sheetname='Data',header=0,datr_parser=True)\n",
    "    temp = pd.DataFrame.empty\n",
    "    for col in cols:\n",
    "         temp = df_interpolated.loc[:col].interpolate(method='linear')\n",
    "         df_interpolated.loc[:,col] = temp\n",
    "    \n",
    "    writer = pd.ExcelWriter(dirs_excel[dir_model_country]+'Combined_hardcoded.xlsx')\n",
    "    df_interpolated.to_excel(writer,'Data',index=False)\n",
    "    \n",
    "    if re_turn == True:\n",
    "        return df_interpolated.loc[:,[cols]]\n",
    "    \n",
    "##Interpolation on coloumn of data\n",
    "##Interpolation on Changes: divides data points by three and spreads across the preceeding two months\n",
    "def Interpolation_Level_Changes(cols,combined_excel_file_path, re_turn = False):\n",
    "    df_uninterpolated = pd.read_excel(combined_excel_file_path+'.xlsx',sheetname='Data',header=0,datr_parser=True)\n",
    "    df_interpolated = df_uninterpolated\n",
    "    temp = pd.DataFrame.empty\n",
    "    \n",
    "    for col in cols:\n",
    "        temp = df_uninterpolated.loc[:,col]\n",
    "        temp = temp[::,-1].interpolate(method='zero', limit_direction='forward')[::-1].div(3)\n",
    "        df_interpolated.loc[:,col]= temp\n",
    "    \n",
    "    writer = pd.ExcelWriter(combined_excel_file_path+'_hardcoded.xlsx')\n",
    "    df_interpolated.to_excel(writer,'Data',index=False)\n",
    "    \n",
    "    if re_turn == True:\n",
    "        return df_interpolated.loc[:,[cols]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Savitzky-Golay Filters Smoothing\n",
    "##sends it from excel file to excel file\n",
    "##intended to work on raw files, to be used before sending to combined\n",
    "\n",
    "\n",
    "#Method\n",
    "##UnSavgolfiltered values are transformed by subtracting the min value from all values\n",
    "##train savgol filter on data : window size chosen to be a factor 2 bigger than polyorder\n",
    "## Finds unit value of Positivity/negativity in SAVGOl filter betwen t1 and t0.\n",
    "##This value (+1/-1) is then multiplied by the corresponding unSavgolfiltered values\n",
    "##Allows one to distinguish between high and rising and high and dipping.\n",
    "\n",
    "def Savgol_filter(out_col,dir_model_country,windowlength=9, polyorder=3,save_xlsx= False, re_turn_graph = False):\n",
    "    \n",
    "    dir_bus_US = 'C:/Users/Rilwa/Desktop/Algorthmic Trading Platform/Business Cycles/US/Data/'\n",
    "    dir_pred_UK = \"\"\n",
    "    dir_pred_JPN = \"\"\n",
    "    \n",
    "    dirs_excel = {'dir_bus_US':dir_bus_US,\n",
    "                        'dir_pred_UK':dir_pred_UK,\n",
    "                        'dir_pred_JPN': dir_pred_JPN}\n",
    "    \n",
    "    path = dirs_excel[dir_model_country]+out_col\n",
    "    \n",
    "    df = pd.read_excel(path+'.xlsx', sheetname= 'Data', parse_dates = False)\n",
    "    \n",
    "    savgol_signal = pd.DataFrame(signal.savgol_filter(df.loc[:,'norm'], window_length=windowlength, polyorder=polyorder))\n",
    "    df['norm'+\" Savgol_filtered\"] = savgol_signal\n",
    "    signal_change_per_period = savgol_signal.diff()\n",
    "    signal_change_per_period = signal_change_per_period.applymap(lambda x: 1-(2*(x<=0)))\n",
    "    df[out_col] = df['norm'] - (df['norm'].min())   \n",
    "    df[out_col] = df[out_col] * signal_change_per_period.iloc[:,0]\n",
    "    \n",
    "    if save_xlsx == True:\n",
    "        writer = pd.ExcelWriter(path+'_hardcoded.xlsx')\n",
    "        df.to_excel(writer,'Data',index=False)\n",
    "\n",
    "    if re_turn_graph == True:\n",
    "        trace0 = go.Scatter(x= np.arange(len(savgol_signal)-1),y=savgol_signal.as_matrix().flatten())\n",
    "        trace1= go.Scatter(x= np.arange(len(savgol_signal)-1), y=df['norm'].as_matrix().flatten())\n",
    "        trace2 = go.Scatter(x= np.arange(len(savgol_signal)-1), y=df[out_col].as_matrix().flatten())\n",
    "        data = [trace0,trace1,trace2]\n",
    "        layout = dict(title = out_col)\n",
    "        py.offline.iplot({\"data\":data, \"layout\": layout})\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
